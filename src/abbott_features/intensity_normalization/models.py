"""Time- and z-decay-normalization models."""

import copy
import json
import os
import pickle
from abc import ABC, abstractmethod
from collections.abc import Mapping, Sequence
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Callable, TypeAlias

import dask
import numpy as np
import pandas as pd
import polars as pl
import xarray as xr
from numpy.typing import ArrayLike, NDArray
from scipy.optimize import curve_fit
from typing_extensions import Self, Unpack

from abbott_features.features.types import SpatialImage

Parameters: TypeAlias = tuple[float, ...]
ModelCallable: TypeAlias = Callable[[NDArray[Any], Unpack[Parameters]], NDArray[Any]]


def to_numpy_or_xarray(X):
    pass


def linear_model(X: ArrayLike, intercept: float, *coef: float) -> NDArray[Any]:
    """Linear model of the form:

    y = b0 + b1 * x1 + b2 * x2 ... + an * xn
    """
    return intercept + np.sum(X * coef, axis=1)


def exponential_model_glm(X: ArrayLike, A0: float, *coef: float) -> NDArray[Any]:
    """Exponential model of the form:

    y = A0 * (np.exp(b1 * x1) * np.exp(b2 * x2) * ... + np.exp(bn * xn))
    """
    return A0 * np.prod(np.exp(X * coef), axis=1)


def exponential_model_with_offset(
    X: ArrayLike, A0: float, C: float, *coef: float
) -> NDArray[Any]:
    """Exponential model with an offset term.

    y = A0 * (np.exp(b1 * x1) * np.exp(b2 * x2) * ... * np.exp(bn * xn)) + C
    """
    return A0 * np.prod(np.exp(X * coef), axis=1) + C


def power_law(X: ArrayLike, A0: float, *coef: float) -> NDArray[Any]:
    """Power law model of the form:

    y = A0 * (x1**b1 * x2**b2 * ... * xn**bn)
    """
    return A0 * np.prod(X**coef, axis=1)


def power_law_with_offset(
    X: ArrayLike, A0: float, C: float, *coef: float
) -> NDArray[Any]:
    """Power law model of the form:

    y = A0 * (x1**b1 * x2**b2 * ... * xn**bn) + C
    """
    return A0 * np.prod(X**coef, axis=1) + C


CALLABLES: dict[str, ModelCallable] = {
    "linear": linear_model,
    "exponential_no_offset": exponential_model_glm,
    "exponential": exponential_model_with_offset,
    "power_law": power_law,
    "power_law_with_offset": power_law_with_offset,
}


# TODO: Currently str(Model) is almost the same as repr(Model), either have a more
# concice __str__ representation or go all in with __repr__.
@dataclass
class Model(ABC):
    """Subclasses need to call both X, y = super().validate(X, y) and super().fit(X, y)

    in their fit method.
    """

    _callable_key: str = field(repr=False)
    _params: Parameters | None = field(default=None, repr=False)
    _feature_names: list[str] | None = field(default=None, repr=True)
    _target_name: str | None = field(default=None, repr=False)
    loss: str = "linear"

    def validate(self, X: ArrayLike, y: ArrayLike):
        if isinstance(X, pl.DataFrame):
            X = X.to_pandas()
        if isinstance(y, pl.Series):
            y = y.to_pandas()
        return X, y

    @abstractmethod
    def fit(self, X: ArrayLike, y: ArrayLike) -> Self:
        if isinstance(X, pd.DataFrame):
            self._feature_names = list(X.columns)
        if isinstance(y, pd.Series):
            self._target_name = y.name

    @property
    @abstractmethod
    def y_intercept(self) -> float:
        pass

    @property
    def params(self) -> Parameters:
        if self._params is None:
            raise RuntimeError(
                "Can't access `self.params` befor the model has been fitted!"
            )
        return self._params

    def predict(self, X: NDArray[Any]) -> NDArray[Any]:
        return CALLABLES[self._callable_key](X, *self.params)

    def grid_predict(self, Xx: tuple[NDArray[Any], NDArray[Any]]) -> NDArray[Any]:
        assert len(Xx) == 2 and isinstance(
            Xx, tuple | list
        ), "pass a list generated by `np.meshgrid` on which to evaluate."
        shape = Xx[0].shape
        X = np.vstack([Xx[0].flatten(), Xx[1].flatten()]).T
        y = self.predict(X)
        return y.reshape(shape)

    def _correction_factor(self, X: NDArray[Any]) -> NDArray[Any]:
        return 1 / self.predict(X) * self.y_intercept

    def _grid_correction_factor(
        self, Xx: tuple[NDArray[Any], NDArray[Any]]
    ) -> NDArray[Any]:
        return 1 / self.grid_predict(Xx) * self.y_intercept

    def correct(self, X: NDArray[Any], y: NDArray[Any]) -> NDArray[Any]:
        return y * self._correction_factor(X)

    def grid_correct(
        self, Xx: tuple[NDArray[Any], NDArray[Any]], yy: NDArray[Any]
    ) -> NDArray[Any]:
        return yy * self._grid_correction_factor(Xx)

    def save(
        self,
        file_name: str | None = None,
        directory: str | Path = Path("."),
        mode="xb",
        write_params_json=True,
    ) -> None:
        if file_name is None:
            # file_name = f"{self.__class__.__name__}"
            file_name = str(self)

        full_out_file = Path(directory) / f"{file_name}.pkl"
        print(f"Saving model to {full_out_file}")
        full_out_file.parent.mkdir(exist_ok=True)
        with open(full_out_file, mode=mode) as f:
            pickle.dump(self, f)
        if write_params_json:
            full_json_file = full_out_file.parent / f"{full_out_file.stem}.json"
            print(f"Saving parametrs to {full_json_file}")
            with open(full_json_file, mode="w") as f:
                d_write = {k: _numpy_to_list(v) for k, v in asdict(self).items()}
                json.dump(d_write, f)

    @classmethod
    def load(cls, full_file_path: str | Path):
        with open(full_file_path, "rb") as f:
            model = pickle.load(f)
        return model

    def __str__(self) -> str:
        class_name = self.__class__.__name__
        repr_fields = {
            k: getattr(self, k) for k, v in self.__dataclass_fields__.items() if v.repr
        }
        out = (
            class_name
            + "("
            + ", ".join(
                [
                    f"{_package_repr_key(k)}={_package_repr_value(v)}"
                    for k, v in repr_fields.items()
                ]
            )
            + ")"
        )
        return out


def _numpy_to_list(arr: NDArray) -> list:
    if isinstance(arr, np.ndarray):
        return list(arr)
    return arr


def _package_repr_value(obj: Sequence, sep=";") -> str:
    if isinstance(obj, Sequence) and not isinstance(obj, str):
        return sep.join([str(e) for e in obj])
    return str(obj)


def _package_repr_key(s: str) -> str:
    mapping = {"_feature_names": "features", "_target_name": "target"}
    return mapping.get(s, s)


@dataclass
class Linear(Model):
    _callable_key: str = field(default="linear", repr=False)

    def fit(self, X: NDArray[Any], y: NDArray[Any]) -> Self:
        X, y = super().validate(X, y)
        super().fit(X, y)
        self._params, _ = curve_fit(
            CALLABLES[self._callable_key],
            X,
            y,
            loss=self.loss,
            method="trf",
            p0=np.ones(X.shape[1] + 1),
        )
        return self

    @property
    def y_intercept(self) -> float:
        return self.params[0]


@dataclass
class LogLinear(Model):
    _callable_key: str = field(default="linear", repr=False)

    def fit(self, X: NDArray[Any], y: NDArray[Any]) -> Self:
        X, y = super().validate(X, y)
        super().fit(X, y)
        self._params, _ = curve_fit(
            CALLABLES[self._callable_key],
            X,
            np.log(np.clip(y, 1e-6, None)),
            loss=self.loss,
            method="trf",
            p0=np.ones(X.shape[1] + 1),
        )
        return self

    def predict(self, X: NDArray[Any]) -> NDArray[Any]:
        return np.exp(CALLABLES[self._callable_key](X, *self.params))

    @property
    def y_intercept(self) -> float:
        return np.exp(self.params[0])


@dataclass
class ExpFitLinear(Model):
    _callable_key: str = field(default="exponential_no_offset", repr=False)

    def fit(self, X: NDArray[Any], y: NDArray[Any]):
        X, y = super().validate(X, y)
        super().fit(X, y)
        # Fit a linear model on log-transformed data.
        lin_params, _ = curve_fit(
            linear_model,
            X,
            np.log(np.clip(y, 1e-6, None)),
            loss=self.loss,
            method="trf",
            p0=np.ones(X.shape[1] + 1),
        )
        # Transform the first coefficient by applying `np.exp`
        self._params = (np.exp(lin_params[0]), *lin_params[1:])
        return self

    @property
    def y_intercept(self) -> float:
        return self.params[0]


@dataclass
class ExpNoOffset(Model):
    _callable_key: str = field(default="exponential_no_offset", repr=False)

    def fit(self, X: NDArray[Any], y: NDArray[Any]):
        X, y = super().validate(X, y)
        super().fit(X, y)
        # Fit an `ExpFitLinear` to find initialization parameters -> better convergence.
        mdl = ExpFitLinear().fit(X, y)
        # Then fit the actual model.
        self._params, _ = curve_fit(
            CALLABLES[self._callable_key],
            X,
            y,
            loss=self.loss,
            method="trf",
            p0=mdl.params,
        )
        return self

    @property
    def y_intercept(self) -> float:
        return self.params[0]


@dataclass
class Exp(Model):
    _callable_key: ModelCallable = field(default="exponential", repr=False)
    offset_init: int = field(default=0, repr=False)
    pos_offset: bool = field(default=False, repr=True)

    def fit(self, X: NDArray[Any], y: NDArray[Any]) -> Self:
        X, y = super().validate(X, y)
        super().fit(X, y)
        # Estimate initial parameters using an `ExponentialModelNoOffset`.
        mdl = ExpNoOffset().fit(X, y)
        p0 = np.array([mdl.params[0], self.offset_init, *mdl.params[1:]])
        lower_bounds = np.array([-np.inf for _ in range(len(p0))])
        if self.pos_offset:
            lower_bounds[1] = 0.0
        upper_bounds = np.array([np.inf for _ in range(len(p0))])
        bounds = (lower_bounds, upper_bounds)
        # Then fit the actual model.
        self._params, _ = curve_fit(
            CALLABLES[self._callable_key],
            X,
            y,
            loss=self.loss,
            method="trf",
            p0=p0,
            bounds=bounds,
        )
        return self

    @property
    def y_intercept(self) -> float:
        return self.params[0] + self.params[1]


def fit_model_to_df(
    df: pd.DataFrame | pl.DataFrame,
    channel: str,
    model: Model,
    X_columns: Sequence[str] = ("MediumPath", "EmbryoPath"),
    y_column: str = "Mean",
    n_samples: int | None = None,
    in_place: bool = False,
) -> Model | None:
    for column in [*list(X_columns), y_column]:
        assert column in df, f"column `{column}` not in `df`"

    if in_place:
        model_copy = model
    else:
        model_copy = copy.deepcopy(model)

    if isinstance(df, pl.DataFrame):
        df = df.to_pandas()

    df_channel = df[df.channel == channel]
    if n_samples:
        df_sample = df_channel.sample(n_samples)
    else:
        df_sample = df_channel
    X = df_sample[list(X_columns)]
    y = df_sample[y_column]
    try:
        model_copy.fit(X, y)
    except RuntimeError as e:
        print(e)
        print(f"no convergance for `{model!s}` in channel `{channel}`")
        return None
    return model_copy


def lazy_apply_model_to_channel(
    model: Model,
    channel_si: SpatialImage,
    label_si: SpatialImage | None = None,
    correction_factor_clip_range: tuple[float, float] | None = (0.0, 50.0),
) -> SpatialImage:
    # Convert to dask-array
    channel_si_da = channel_si.chunk("auto")
    if label_si is not None:
        label_si_da = label_si.chunk("auto")

    if model._feature_names == ["Centroid-z"]:
        correction_factors = model._correction_factor(
            channel_si_da.coords["z"].expand_dims(["_"], -1)
        )
        if isinstance(correction_factors, np.ndarray):
            correction_factors = xr.DataArray(
                correction_factors, coords={"z": channel_si_da.coords["z"]}
            )
        return (channel_si_da * correction_factors).rename("image")

    elif model._feature_names == ["MediumPath", "EmbryoPath"]:
        assert (
            label_si is not None
        ), "Provide embryo segmentation for 2-step correction."
        embryo_path_si = (label_si_da).cumsum(dim="z") * label_si_da.attrs[
            "scale_dict"
        ]["z"]
        medium_path_si = (~label_si_da.astype(bool)).cumsum(
            dim="z"
        ) * label_si_da.attrs["scale_dict"]["z"]

        with dask.config.set(**{"array.slicing.split_large_chunks": False}):
            X = np.stack(
                [medium_path_si.data.flatten(), embryo_path_si.data.flatten()]
            ).T
            # X = np.concatenate([medium_path_si, embryo_path_si]).reshape(2, -1).T
            correction_factors = model._correction_factor(X).T.reshape(
                channel_si_da.shape
            )
            if correction_factor_clip_range is not None:
                lb, ub = correction_factor_clip_range
                correction_factors[
                    np.logical_or(correction_factors < lb, correction_factors > ub)
                ] = ub
        return (channel_si_da * correction_factors).rename("image")

    else:
        raise ValueError(f"Unkown model feature names: {model._feature_names=}")


def apply_z_decay_models(
    models: Mapping[str, Model | None] | None,
    channel_si: SpatialImage,
    two_step_label_si: SpatialImage,
    correction_factor_clip_range: tuple[float, float] | None = (0.0, 50.0),
) -> SpatialImage:
    if models is None:
        return channel_si

    model = models[channel_si.name]
    if model is None:
        channel_si_corr = channel_si
    else:
        assert model._feature_names is not None, "Model not fit."
        if len(model._feature_names) != 2:
            two_step_label_si = None
        channel_si_corr = lazy_apply_model_to_channel(
            model, channel_si, two_step_label_si, correction_factor_clip_range
        )

    channel_si_corr.name = channel_si.name

    return channel_si_corr


def apply_t_decay_factor(
    channel_si: SpatialImage,
    correction_factors_df: pl.DataFrame,
    ROI_id: str,
    maintain_image_dtype: bool = True,
) -> SpatialImage:
    """Apply time decay correction factor to a channel spatial image."""
    correction_factor = (
        correction_factors_df.filter(
            (pl.col("channel") == channel_si.name) & (pl.col("ROI") == int(ROI_id))
        )
        .select(pl.col("correctionFactor"))
        .item()
    )

    # Check if the correction factor is not nan
    if np.isnan(correction_factor):
        raise ValueError(f"Correction factor for channel {channel_si.name} is NaN.")

    channel_si_out = channel_si * correction_factor
    if maintain_image_dtype:
        channel_si_out = channel_si_out.astype(channel_si.dtype)

    channel_si_out.name = channel_si.name
    return channel_si_out


def write_models(
    models: dict,
    root: Path | str,
    write_params_json: bool = True,
    overwrite: bool = True,
):
    """Write a nested dict to a nested file structure."""
    for key, value in models.items():
        # Create a directory for the current key
        current_path = os.path.join(root, str(key))
        os.makedirs(current_path, exist_ok=True)
        if not overwrite and os.path.exists(current_path):
            continue
        # Recursively write sub-dictionaries or write the value to a file
        if isinstance(value, dict):
            write_models(value, current_path)
        elif isinstance(value, Model):
            value.save(
                directory=current_path,
                file_name="model",
                mode="wb",
                write_params_json=write_params_json,
            )


def read_models(root: Path | str):
    """Read a nested file structure to a nested dict."""
    result = {}
    if not Path(root).exists():
        raise FileNotFoundError(f"Path {root} does not exist.")
    for item in os.listdir(root):
        item_path = os.path.join(root, item)
        if os.path.isdir(item_path):
            # Recursively process subdirectories
            result[item] = read_models(item_path)
        elif os.path.isfile(item_path) and item_path.endswith(".pkl"):
            result = Model.load(item_path)
    return result
